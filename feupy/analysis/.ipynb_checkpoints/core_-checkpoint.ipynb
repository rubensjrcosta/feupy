{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75dd3a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/born-again/anaconda3/envs/gammapy-1.1/lib/python3.9/site-packages/pydantic/_migration.py:290: UserWarning: `pydantic.utils:deep_update` has been removed. We are importing from `pydantic.v1.utils:deep_update` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n",
    "\"\"\"Session class driving the high level interface API\"\"\"\n",
    "import logging\n",
    "import yaml\n",
    "import pandas as pd \n",
    "import json\n",
    "from gammapy.modeling.models import Model\n",
    "from regions import CircleSkyRegion\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import time\n",
    "\n",
    "from astropy.units import Quantity\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "\n",
    "# from pydantic import BaseModel\n",
    "from pydantic.utils import deep_update\n",
    "\n",
    "from gammapy.utils.units import energy_unit_format\n",
    "\n",
    "from gammapy.utils.pbar import progress_bar\n",
    "from gammapy.utils.scripts import make_path, read_yaml\n",
    "\n",
    "from gammapy.datasets import (\n",
    "    Datasets,  \n",
    "    MapDataset, \n",
    "    FluxPointsDataset, \n",
    "    SpectrumDatasetOnOff, \n",
    "    SpectrumDataset,\n",
    ")\n",
    "from feupy.utils.datasets import flux_points_dataset_from_table\n",
    "\n",
    "\n",
    "from gammapy.utils.units import energy_unit_format\n",
    "\n",
    "\n",
    "from gammapy.estimators import FluxPoints, SensitivityEstimator\n",
    "from gammapy.maps import Map, MapAxis, RegionGeom, WcsGeom\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel, \n",
    "    Models,\n",
    "    Model,\n",
    "    DatasetModels, \n",
    "    FoVBackgroundModel, \n",
    "    Models, \n",
    "    SkyModel, \n",
    "    ExpCutoffPowerLawSpectralModel\n",
    ")\n",
    "\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.estimators import (\n",
    "    ExcessMapEstimator,\n",
    "    FluxPointsEstimator,\n",
    "    LightCurveEstimator,\n",
    ")\n",
    "from gammapy.makers import (\n",
    "    FoVBackgroundMaker,\n",
    "    MapDatasetMaker,\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    SpectrumDatasetMaker,\n",
    ")\n",
    "\n",
    "from gammapy.data import Observation\n",
    "\n",
    "from gammapy.stats import WStatCountsStatistic\n",
    "from feupy.analysis.simulation.observations import sensitivity_estimator\n",
    "from feupy.analysis.simulation.stats import StatisticalUtilityFunctions as stats\n",
    "\n",
    "from feupy.utils.string_handling import name_to_txt\n",
    "from feupy.utils.datasets import cut_energy_table_fp, write_datasets, read_datasets\n",
    "\n",
    "# from feupy.analysis import CounterpartsAnalysisConfig, SimulationConfig, CTAObservationAnalysisConfig\n",
    "from feupy.cta.irfs import Irfs\n",
    "from feupy.utils.coordinates import skcoord_to_dict, skcoord_config_to_skcoord\n",
    "\n",
    "from feupy.analysis.config import CounterpartsConfig, SimulationConfig\n",
    "\n",
    "from feupy.plotters import *\n",
    "\n",
    "# from feupy.catalog.config import *\n",
    "from feupy.plotters import *\n",
    "from feupy.roi import ROI\n",
    "from feupy.target import Target\n",
    "\n",
    "from feupy.catalog.pulsar.atnf import SourceCatalogATNF\n",
    "\n",
    "from gammapy.data import FixedPointingInfo, PointingMode\n",
    "\n",
    "from astropy.io.fits.verify import VerifyWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933f67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"Counterparts\", \"Simulation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1144f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class Counterparts:\n",
    "    \"\"\"Config-driven high level simulation interface.\n",
    "\n",
    "    It is initialized by default with a set of configuration parameters and values declared in\n",
    "    an internal high level interface model, though the user can also provide configuration\n",
    "    parameters passed as a nested dictionary at the moment of instantiation. In that case these\n",
    "    parameters will overwrite the default values of those present in the configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict or `CounterpartsConfig`\n",
    "        Configuration options following `CounterpartsConfig` schema\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.config.set_logging()\n",
    "        self.roi = self._create_roi()\n",
    "        self.roi.get_catalogs()\n",
    "        self.target = None\n",
    "        self.catalogs = None\n",
    "        self.datasets = None\n",
    "        self.sources = None\n",
    "        self.pulsars = None\n",
    "        self.df_sep = None\n",
    "        self.leg_style = None\n",
    "        \n",
    "    def _set_leg_style(self):\n",
    "        datasets_names = self.datasets.names\n",
    "        models_names = self.datasets.models.names\n",
    "        for pulsar in self.pulsars:\n",
    "            name = pulsar.name\n",
    "            datasets_names.append(name)\n",
    "        leg_style = set_leg_style(\n",
    "            leg_style ={}, \n",
    "            datasets_names=datasets_names, \n",
    "            models_names=models_names\n",
    "        )\n",
    "        self.leg_style = leg_style\n",
    "        \n",
    "        \n",
    "    def _create_target(self):\n",
    "        \"\"\"Create the target.\"\"\"\n",
    "        log.debug(\"Creating target.\")\n",
    "        target_settings = self.config.roi.target\n",
    "        name = target_settings.name\n",
    "        pos_ra = target_settings.position.lon\n",
    "        pos_dec = target_settings.position.lat\n",
    "        model = Model.from_dict(target_settings.model)\n",
    "        return Target(name, pos_ra, pos_dec, spectral_model=model.spectral_model)\n",
    "    \n",
    "    def _create_roi(self):\n",
    "        \"\"\"Create the target.\"\"\"\n",
    "        log.debug(\"Creating target.\")\n",
    "        target = self._create_target()\n",
    "        self.target = target\n",
    "        on_region_radius = self.config.roi.radius\n",
    "        return ROI(target, on_region_radius )\n",
    "        \n",
    "    @property\n",
    "    def config(self):\n",
    "        \"\"\"Simulation configuration (`CounterpartsConfig`)\"\"\"\n",
    "        return self._config\n",
    "\n",
    "    @config.setter\n",
    "    def config(self, value):\n",
    "        if isinstance(value, dict):\n",
    "            self._config = CounterpartsConfig(**value)\n",
    "        elif isinstance(value, CounterpartsConfig):\n",
    "            self._config = value\n",
    "        else:\n",
    "            raise TypeError(\"config must be dict or CounterpartsConfig.\")\n",
    "\n",
    "    @property\n",
    "    def models(self):\n",
    "        if not self.datasets:\n",
    "            raise RuntimeError(\"No datasets defined. Impossible to set models.\")\n",
    "        return self.datasets.models\n",
    "\n",
    "    @models.setter\n",
    "    def models(self, models):\n",
    "        self.set_models(models, extend=False)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_energy_axis(config_axis_energy, per_decade=True):\n",
    "        return MapAxis.from_energy_bounds(        \n",
    "            energy_min=config_axis_energy.min, \n",
    "            energy_max=config_axis_energy.max, \n",
    "            nbin=config_axis_energy.nbins, \n",
    "            per_decade=per_decade, \n",
    "            name=config_axis_energy.name,\n",
    "            )\n",
    "    \n",
    "    def run(self):\n",
    "        self._get_datasets()\n",
    "        self._set_leg_style()\n",
    "        \n",
    "    def _get_datasets(self):\n",
    "        \"\"\"\n",
    "        Select a catalog subset (only sources within a region of interest)\n",
    "        \"\"\"\n",
    "        _catalogs = self.roi.catalogs\n",
    "        self.pulsars = self.roi.pulsars\n",
    "        pulsars = self.pulsars\n",
    "        e_ref_min = self.config.energy_range.min\n",
    "        e_ref_max = self.config.energy_range.max\n",
    "\n",
    "        \n",
    "        datasets = Datasets() # global datasets object\n",
    "        models = Models()  # global models object\n",
    "        sources = [] # global sources object\n",
    "        catalogs = []\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', VerifyWarning)\n",
    "            for catalog in _catalogs:\n",
    "                indexes = []\n",
    "                cat_tag = catalog.tag\n",
    "                for source in catalog: \n",
    "                    source_name = source.name \n",
    "                    index = source.row_index\n",
    "                    if cat_tag == SourceCatalogATNF.tag:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            flux_points = source.flux_points\n",
    "\n",
    "                            spectral_model = source.spectral_model()\n",
    "                            spectral_model_tag = spectral_model.tag[1]\n",
    "\n",
    "                            if cat_tag == 'gamma-cat' or cat_tag == 'hgps':\n",
    "                                dataset_name = f'{source_name}: {cat_tag}'\n",
    "                            else: dataset_name = source_name\n",
    "\n",
    "                            file_name = name_to_txt(dataset_name)\n",
    "\n",
    "                            model = SkyModel(\n",
    "                                name=f\"{file_name}_{spectral_model_tag}\",\n",
    "                                spectral_model=spectral_model,\n",
    "                                datasets_names=dataset_name\n",
    "                            )\n",
    "\n",
    "                            dataset = FluxPointsDataset(\n",
    "                                models=model,\n",
    "                                data=flux_points, \n",
    "                                name=dataset_name   \n",
    "                            )\n",
    "                            \n",
    "                            if any([e_ref_min !=  None, e_ref_max !=  None]):\n",
    "                                dataset = cut_energy_table_fp(dataset, e_ref_min, e_ref_max) \n",
    "\n",
    "                            models.append(model)  # Add the model to models()\n",
    "                            datasets.append(dataset)\n",
    "                            sources.append(source)\n",
    "                        except Exception as error:\n",
    "                            indexes.append(index)\n",
    "                            # By this way we can know about the type of error occurring\n",
    "                            print(f'The error is: ({source_name}) {error}') \n",
    "                if len(indexes)>0:\n",
    "                    if len(indexes)==1:\n",
    "                        catalog.table.remove_row(indexes[0])\n",
    "                    else: catalog.table.remove_rows(indexes)\n",
    "\n",
    "                if len(catalog.table)>0:\n",
    "                    catalogs.append(catalog)\n",
    "            datasets.models = models\n",
    "            self.datasets = datasets\n",
    "            self.sources = sources\n",
    "            self.catalogs = catalogs\n",
    "            \n",
    "            target_pos = self.roi.target.position\n",
    "            _sources = sources.copy()\n",
    "            _sources.extend(pulsars)\n",
    "            \n",
    "            dict_sep = self.roi.get_dict_sep(target_pos, _sources, opt=\"pos_dict\")\n",
    "            self.config.roi.dict_sep = dict_sep\n",
    "            self.df_sep = self.roi.get_df_sep(dict_sep) \n",
    "        \n",
    "            print(f\"Total number of gamma sources: {len(self.sources)}\")\n",
    "            print(f\"Total number of flux points tables: {len(self.datasets)}\")\n",
    "            print(f\"Total number of pulsars: {len(self.pulsars)}\")\n",
    "             \n",
    "                \n",
    "    def create_analysis_name(self): \n",
    "        \"\"\" ... \"\"\"\n",
    "        ss = f\"{self.config.target.name}\"\n",
    "        ss += \"_roi_{:.2f}\".format(self.roi.radius).replace(' ', '')\n",
    "        if e_ref_min is None: ss += \"\"\n",
    "        else: ss += \"_e_ref_min_{}\".format(energy_unit_format(e_ref_min).replace(' ', ''))\n",
    "        if e_ref_max is None: ss += \"\"\n",
    "        else: ss += \"_e_ref_max_{}\".format(energy_unit_format(e_ref_max).replace(' ', ''))\n",
    "        return ss\n",
    "    \n",
    "#     def create_analysis_path(self): \n",
    "#         \"\"\" ... \"\"\"\n",
    "#         return Path(f\"analysis_counterparts/{self.create_analysis_name()}\")\n",
    "\n",
    "    def set_models(self, models, extend=True):\n",
    "        \"\"\"Set models on datasets.\n",
    "        Adds `FoVBackgroundModel` if not present already\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : `~gammapy.modeling.models.Models` or str\n",
    "            Models object or YAML models string\n",
    "        extend : bool\n",
    "            Extend the exiting models on the datasets or replace them.\n",
    "        \"\"\"\n",
    "        if not self.datasets or len(self.datasets) == 0:\n",
    "            raise RuntimeError(\"Missing datasets\")\n",
    "\n",
    "        log.info(\"Reading model.\")\n",
    "        if isinstance(models, str):\n",
    "            models = Models.from_yaml(models)\n",
    "        elif isinstance(models, Models):\n",
    "            pass\n",
    "        elif isinstance(models, DatasetModels) or isinstance(models, list):\n",
    "            models = Models(models)\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid type: {models!r}\")\n",
    "\n",
    "        if extend:\n",
    "            models.extend(self.datasets.models)\n",
    "\n",
    "        self.datasets.models = models\n",
    "\n",
    "\n",
    "        log.info(models)\n",
    "\n",
    "    def write_datasets(self, overwrite=True, path_file=None):\n",
    "        \"\"\"Write Datasets and Models to YAML file.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            overwrite : bool, optional\n",
    "                Overwrite existing file. Default is True.  \n",
    "            \"\"\"\n",
    "        \n",
    "        if path_file is None:\n",
    "            path_file = Path(f\"{self.config.create_analysis_path()}/datasets\")\n",
    "        write_datasets(self.datasets, path_file, overwrite)\n",
    "    \n",
    "    def read_datasets(self, path_file=None):\n",
    "        \"\"\"Read Datasets and Models from YAML file.\"\"\"\n",
    "\n",
    "        if path_file is None:\n",
    "            path_file = Path(f\"{self.config.create_analysis_path()}/datasets\")\n",
    "        return read_datasets(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15aded6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba3e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class Simulation:\n",
    "    \"\"\"Config-driven high level simulation interface.\n",
    "\n",
    "    It is initialized by default with a set of configuration parameters and values declared in\n",
    "    an internal high level interface model, though the user can also provide configuration\n",
    "    parameters passed as a nested dictionary at the moment of instantiation. In that case these\n",
    "    parameters will overwrite the default values of those present in the configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict or `SimulationConfig`\n",
    "        Configuration options following `SimulationConfig` schema\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.config.set_logging()\n",
    "    \n",
    "        self._ctao_perf = Irfs\n",
    "        self._ctao_perf.get_irfs(self.config.observations.irfs.opt)\n",
    "#         self.observation = self._create_observation()\n",
    "#         self.geom = self._create_geometry()\n",
    "#         self.dataset_empty = self._create_spectrum_dataset_empty()\n",
    "        \n",
    "#         self._obs = self.config.observations\n",
    "#         self._obs_params = self.config.observations.parameters\n",
    "#         self._obs_target = self.config.observations.target\n",
    "#         self._obs_irfs = self.config.observations.irfs\n",
    "#         self._datasets = self.config.datasets\n",
    "     \n",
    "        \n",
    "    def _create_observation(self):\n",
    "        \"\"\"Create an observation.\"\"\"\n",
    "        log.debug(\"Creating observation.\")\n",
    "        observations_settings = self.config.observations\n",
    "        position = skcoord_config_to_skcoord(observations_settings.target.position)\n",
    "        position_angle = observations_settings.pointing.angle\n",
    "        separation = observations_settings.parameters.offset\n",
    "        pointing_position = self._create_pointing_position(position, position_angle, separation)\n",
    "        pointing = self._create_pointing(pointing_position)\n",
    "        print(pointing)\n",
    "        livetime = observations_settings.parameters.livetime\n",
    "        irfs = self._ctao_perf.irfs\n",
    "        location = self._ctao_perf.obs_loc\n",
    "        obs = Observation.create(pointing=pointing, livetime=livetime, irfs=irfs, location=location)\n",
    "        print(obs)\n",
    "        return obs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_pointing_position(position, position_angle, separation):\n",
    "        \"\"\"Create the pointing position\"\"\"\n",
    "        return position.directional_offset_by(position_angle, separation)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_pointing(pointing_position):\n",
    "        \"\"\"Create the pointing.\"\"\"\n",
    "        return FixedPointingInfo(\n",
    "            mode=PointingMode.POINTING,\n",
    "            fixed_icrs=pointing_position.icrs,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _create_geometry(self):\n",
    "        \"\"\"Create the geometry.\"\"\"\n",
    "        log.debug(\"Creating geometry.\")\n",
    "        geom_settings = self.config.datasets.geom\n",
    "        observations_settings = self.config.observations\n",
    "        axes = [self._make_energy_axis(geom_settings.axes.energy)]\n",
    "        print(axes)\n",
    "        center = skcoord_config_to_skcoord(observations_settings.target.position)\n",
    "        radius = observations_settings.parameters.on_region_radius\n",
    "        region = self._create_on_region(center, radius)\n",
    "        print(region)\n",
    "        geom = RegionGeom.create(region=region, axes=axes)\n",
    "        print(geom)\n",
    "        return geom\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_energy_axis(config_axis_energy, per_decade=True):\n",
    "        \"\"\"Create the energy axis.\"\"\"\n",
    "        energy_axis = MapAxis.from_energy_bounds(        \n",
    "            energy_min=config_axis_energy.min, \n",
    "            energy_max=config_axis_energy.max, \n",
    "            nbin=config_axis_energy.nbins, \n",
    "            per_decade=per_decade, \n",
    "            name=config_axis_energy.name,\n",
    "            )\n",
    "        return energy_axis\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_on_region(center, radius):\n",
    "        \"\"\"Create the region geometry.\"\"\"\n",
    "        return CircleSkyRegion(\n",
    "            center=center, \n",
    "            radius=radius\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _create_spectrum_dataset_empty(self, name=\"obs-0\"):\n",
    "        \"\"\"# Creates a Spectrum Dataset object with zero filled maps.\"\"\"\n",
    "        log.debug(\"Creating a Spectrum Dataset object with zero filled maps.\")\n",
    "        geom = self._create_geometry()\n",
    "        geom_settings = self.config.datasets.geom\n",
    "        energy_axis_true = self._make_energy_axis(geom_settings.axes.energy_true)\n",
    "        print(energy_axis_true)\n",
    "        # Creates a SpectrumDataset object with zero filled maps.\n",
    "        dataset_empty = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=name)\n",
    "        print(dataset_empty)\n",
    "        return dataset_empty\n",
    "    \n",
    "    def _create_dataset_maker(self):\n",
    "        \"\"\"Create the Spectrum Dataset Maker.\"\"\"\n",
    "        log.debug(\"Creating the Spectrum Dataset Maker.\")\n",
    "\n",
    "        datasets_settings = self.config.datasets\n",
    "            \n",
    "        if datasets_settings.type == \"3d\":\n",
    "            maker = MapDatasetMaker(selection=datasets_settings.selection)\n",
    "            \n",
    "        elif datasets_settings.type == \"1d\":\n",
    "            maker_config = {}\n",
    "            if datasets_settings.containment_correction:\n",
    "                maker_config[\n",
    "                    \"containment_correction\"\n",
    "                ] = datasets_settings.containment_correction\n",
    "\n",
    "            maker_config[\"selection\"] = datasets_settings.selection\n",
    "            maker_config[\"use_region_center\"] = datasets_settings.use_region_center\n",
    "            maker = SpectrumDatasetMaker(**maker_config)\n",
    "\n",
    "        return maker\n",
    "    \n",
    "    def _create_safe_mask_maker(self):\n",
    "        \"\"\"Create the Safe Mas kMaker.\"\"\"\n",
    "        log.debug(\"Creating Safe Mas kMaker.\")\n",
    "\n",
    "        safe_mask_selection = self.config.datasets.safe_mask.methods\n",
    "        safe_mask_settings = self.config.datasets.safe_mask.parameters\n",
    "\n",
    "        return SafeMaskMaker(methods=safe_mask_selection, **safe_mask_settings)\n",
    "        \n",
    "        \n",
    "    def set_model_on_dataset(self, random_state=42):\n",
    "        \"\"\"Make maps and datasets for 3d analysis\"\"\"\n",
    "        \n",
    "        observation = self._create_observation()\n",
    "        target_settings = self.config.observations.target\n",
    "        log.info(\"Creating reference dataset and makers.\")\n",
    "\n",
    "        dataset_empty = self._create_spectrum_dataset_empty()\n",
    "        maker = self._create_dataset_maker()\n",
    "        safe_maker = self._create_safe_mask_maker()\n",
    "        \n",
    "        log.info(\"running makers and safing.\")\n",
    "        \n",
    "        dataset = maker.run(dataset_empty, observation)\n",
    "        dataset = safe_maker.run(dataset, observation)\n",
    "\n",
    "        log.info(\"Set the model on the dataset, and fake.\")\n",
    "\n",
    "        dataset.models = Model.from_dict(target_settings.model)\n",
    "        dataset.fake(random_state=random_state)\n",
    "        self.spectrum_dataset = dataset\n",
    "        print(dataset)\n",
    "\n",
    "    @staticmethod    \n",
    "    def _create_safe_spectrum_dataset_onoff(dataset, acceptance, acceptance_off):\n",
    "    # Spectrum dataset for on-off likelihood fitting.\n",
    "        dataset_onoff = SpectrumDatasetOnOff.from_spectrum_dataset(\n",
    "            dataset=dataset, \n",
    "            acceptance=acceptance, \n",
    "            acceptance_off=acceptance_off,\n",
    "        )\n",
    "        dataset_onoff.fake(\n",
    "            random_state='random-seed', \n",
    "            npred_background=dataset.npred_background()\n",
    "        )\n",
    "        return(dataset_onoff)\n",
    "    \n",
    "    def run_onoff(self): \n",
    "        n_obs = self.config.observations.parameters.n_obs\n",
    "        dataset = self.spectrum_dataset\n",
    "        datasets_onoff_settings = self.config.datasets_onoff\n",
    "        stat_settings = self.config.statistics\n",
    "        sens_settings = self.config.sensitivity\n",
    "        alpha = stat_settings.alpha\n",
    "        acceptance = self.config.datasets_onoff.acceptance \n",
    "        acceptance_off = self.config.datasets_onoff.acceptance_off\n",
    "        dataset_onoff = self._create_safe_spectrum_dataset_onoff(dataset, acceptance, acceptance_off)\n",
    "        self.spectrum_dataset_onoff = dataset_onoff\n",
    "        _wstat, wstat_dict = self._compute_wstat(dataset_onoff, alpha)\n",
    "        self.config.statistics.wstat = wstat_dict\n",
    "        self.update_config(self.config)\n",
    "        spectrum = Model.from_dict(self.config.observations.target.model).spectral_model\n",
    "        gamma_min = sens_settings.gamma_min  \n",
    "        n_sigma = sens_settings.n_sigma \n",
    "        bkg_syst_fraction = sens_settings.bkg_syst_fraction\n",
    "        name = f\"sens ({self._ctao_perf.irfs_label})\"\n",
    "        sens, sensitivity_ds = self._compute_sensitivity(spectrum, gamma_min, n_sigma, bkg_syst_fraction, dataset_onoff, sed_type=\"e2dnde\", name=name)\n",
    "        self.sens = sens \n",
    "        self.sensitivity_ds = sensitivity_ds\n",
    "        self.wstat = _wstat\n",
    "        datasets = Datasets()\n",
    "\n",
    "        for idx in range(n_obs):\n",
    "            dataset_onoff.fake(\n",
    "                random_state=idx, \n",
    "                npred_background=dataset.npred_background()\n",
    "            )\n",
    "            dataset_fake = dataset_onoff.copy(name=f\"obs-{idx}\")\n",
    "            dataset_fake.meta_table[\"OBS_ID\"] = [idx]\n",
    "            datasets.append(dataset_fake)\n",
    "        print(datasets)\n",
    "        self.datasets = datasets\n",
    "        \n",
    "    \n",
    "    @staticmethod    \n",
    "    def _compute_wstat(dataset_onoff, alpha):\n",
    "        log.info(\"computing wstatistics.\")\n",
    "        wstat = stats.compute_wstat(dataset_onoff=dataset_onoff, alpha=alpha)\n",
    "        wstat_dict = wstat.info_dict()\n",
    "        wstat_dict[\"n_on\"] = float(wstat_dict[\"n_on\"])\n",
    "        wstat_dict[\"n_off\"] = float(wstat_dict[\"n_off\"])\n",
    "        wstat_dict[\"background\"] = float(wstat_dict[\"background\"])\n",
    "        wstat_dict[\"excess\"] = float(wstat_dict[\"excess\"])\n",
    "        wstat_dict[\"significance\"] = float(wstat_dict[\"significance\"])\n",
    "        wstat_dict[\"p_value\"] = float(wstat_dict[\"p_value\"])\n",
    "        wstat_dict[\"alpha\"] = float(wstat_dict[\"alpha\"])\n",
    "        wstat_dict[\"mu_sig\"] =float(wstat_dict[\"mu_sig\"])\n",
    "\n",
    "        wstat_dict['error'] = float(wstat.error)\n",
    "        wstat_dict['stat_null'] = float(wstat.stat_null)\n",
    "        wstat_dict['stat_max'] = float(wstat.stat_max)\n",
    "        wstat_dict['ts'] = float(wstat.ts)\n",
    "        print(f\"Number of excess counts: {wstat.n_sig}\")\n",
    "        print(f\"TS: {wstat.ts}\")\n",
    "        print(f\"Significance: {wstat.sqrt_ts}\")\n",
    "        return wstat, wstat_dict\n",
    "    \n",
    "    @staticmethod    \n",
    "    def _compute_sensitivity(spectrum, gamma_min, n_sigma, bkg_syst_fraction, dataset_onoff, sed_type=\"e2dnde\", name=\"sens\"):\n",
    "        log.info(\"computing sensitivity.\")\n",
    "        sens, sensitivity_table = sensitivity_estimator(\n",
    "            spectrum=spectrum,\n",
    "            gamma_min=gamma_min, \n",
    "            n_sigma=n_sigma, \n",
    "            bkg_syst_fraction=bkg_syst_fraction, \n",
    "            dataset_onoff=dataset_onoff)\n",
    "\n",
    "        sensitivity_ds = flux_points_dataset_from_table(\n",
    "            sensitivity_table, \n",
    "            reference_model=spectrum.copy(),\n",
    "            sed_type=sed_type,\n",
    "            name=name)\n",
    "\n",
    "        return sens, sensitivity_ds\n",
    "\n",
    "    def fit_model_parameters(self): \n",
    "        datasets = self.datasets\n",
    "        model = Model.from_dict(self.config.observations.target.model)\n",
    "        fitted_parameters, fitted_parameters_dict = self._fit_params(datasets, model)\n",
    "        self.fitted_params = fitted_parameters\n",
    "        print(fitted_parameters_dict)\n",
    "\n",
    "        self.config.statistics.fitted_parameters = fitted_parameters_dict\n",
    "        self.update_config(self.config)\n",
    "        \n",
    "    @staticmethod    \n",
    "    def _fit_params(datasets, model):\n",
    "#         %%time\n",
    "\n",
    "        results = []\n",
    "\n",
    "        fit = Fit()\n",
    "\n",
    "        for dataset in datasets.copy():\n",
    "            dataset.models = model.copy()\n",
    "            result = fit.optimize(dataset)\n",
    "\n",
    "            if result.success:\n",
    "                par_dict = {}\n",
    "                for par in result.parameters.free_parameters:\n",
    "                    par_dict[par.name] = par.quantity\n",
    "                results.append(par_dict)\n",
    "\n",
    "        fitted_params = Table(results).to_pandas()\n",
    "        mean = fitted_params.mean()\n",
    "        uncertainty = fitted_params.std()\n",
    "        fitted_params_dict = {}\n",
    "        for name in list(results[0].keys()):\n",
    "            fitted_params_dict[name] = {\n",
    "                \"mean\": mean[name],\n",
    "                \"uncertainty\": uncertainty[name]\n",
    "            }\n",
    "            print(f\"{name} :\\t {mean[name]:.2e} -+ {uncertainty[name]:.2e}\")\n",
    "    \n",
    "        return fitted_params, fitted_params_dict\n",
    "    \n",
    "\n",
    "  \n",
    "    def fit_joint(self):\n",
    "        datasets = self.datasets\n",
    "        model = Model.from_dict(self.config.observations.target.model)\n",
    "\n",
    "        #Compute flux points\n",
    "        datasets.models = [model]\n",
    "\n",
    "        # fit_joint = Fit(backend='sherpa')\n",
    "        fit_joint = Fit()\n",
    "        fit_result_joint = fit_joint.run(datasets=datasets)\n",
    "        print(fit_result_joint)\n",
    "        self.datasets.models = model\n",
    "        self.config.observations.target.model_fitted = model.to_dict()\n",
    "        print(model.to_dict())\n",
    "        self.update_config(self.config)\n",
    "        \n",
    "    @property\n",
    "    def config(self):\n",
    "        \"\"\"Simulation configuration (`SimulationConfig`)\"\"\"\n",
    "        return self._config\n",
    "\n",
    "    @config.setter\n",
    "    def config(self, value):\n",
    "        if isinstance(value, dict):\n",
    "            self._config = SimulationConfig(**value)\n",
    "        elif isinstance(value, SimulationConfig):\n",
    "            self._config = value\n",
    "        else:\n",
    "            raise TypeError(\"config must be dict or SimulationConfig.\")\n",
    "\n",
    "    @property\n",
    "    def models(self):\n",
    "        if not self.datasets:\n",
    "            raise RuntimeError(\"No datasets defined. Impossible to set models.\")\n",
    "        return self.datasets.models\n",
    "\n",
    "    @models.setter\n",
    "    def models(self, models):\n",
    "        self.set_models(models, extend=False)\n",
    "        \n",
    "    def update_config(self, config):\n",
    "        \"\"\"Update the configuration.\"\"\"\n",
    "        self.config = self.config.update(config=config)\n",
    "        \n",
    "    def set_models(self, models, extend=True):\n",
    "        \"\"\"Set models on datasets.\n",
    "        Adds `FoVBackgroundModel` if not present already\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : `~gammapy.modeling.models.Models` or str\n",
    "            Models object or YAML models string\n",
    "        extend : bool\n",
    "            Extend the exiting models on the datasets or replace them.\n",
    "        \"\"\"\n",
    "        if not self.datasets or len(self.datasets) == 0:\n",
    "            raise RuntimeError(\"Missing datasets\")\n",
    "\n",
    "        log.info(\"Reading model.\")\n",
    "        if isinstance(models, str):\n",
    "            models = Models.from_yaml(models)\n",
    "        elif isinstance(models, Models):\n",
    "            pass\n",
    "        elif isinstance(models, DatasetModels) or isinstance(models, list):\n",
    "            models = Models(models)\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid type: {models!r}\")\n",
    "\n",
    "        if extend:\n",
    "            models.extend(self.datasets.models)\n",
    "\n",
    "        self.datasets.models = models\n",
    "\n",
    "\n",
    "        log.info(models)\n",
    "\n",
    "    def write_datasets(self, overwrite=True, path_file=None):\n",
    "        \"\"\"Write Datasets and Models to YAML file.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            overwrite : bool, optional\n",
    "                Overwrite existing file. Default is True.  \n",
    "            \"\"\"\n",
    "\n",
    "        write_datasets(self.datasets, path_file, overwrite)\n",
    "    \n",
    "    def read_datasets(self, path_file=None):\n",
    "        \"\"\"Read Datasets and Models from YAML file.\"\"\"\n",
    "        return read_datasets(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd551f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
